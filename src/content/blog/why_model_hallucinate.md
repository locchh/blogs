---
title:Â 'WhyÂ modelÂ hallucinate?'
description:Â 'SomeÂ explainationÂ aboutÂ whyÂ modelÂ hallucinate'
pubDate:Â 'JanÂ 24Â 2026'
heroImage:Â ''
---

##Â [OpenAI'sÂ Explanation](https://openai.com/index/why-language-models-hallucinate/)

###Â BecauseÂ theÂ generationÂ processÂ itself

-Â **ProbabilisticÂ nature**:Â AlwaysÂ generatingÂ mostÂ likelyÂ nextÂ word,Â notÂ mostÂ accurate

-Â **PatternÂ matchingÂ vsÂ reasoning**:Â ModelsÂ excelÂ atÂ statisticalÂ patternsÂ butÂ lackÂ trueÂ understandingÂ ofÂ facts

###Â BecauseÂ theÂ trainingÂ andÂ evaluationÂ setup

-Â **BenchmarkÂ gaming**:Â ModelsÂ optimizedÂ forÂ testÂ performanceÂ ratherÂ thanÂ truthfulness

-Â **ConfidenceÂ rewarded**:Â SystemsÂ preferÂ confidentÂ wrongÂ answersÂ overÂ uncertainÂ admissions

###Â BecauseÂ theÂ dataÂ andÂ knowledgeÂ limitations

-Â **KnowledgeÂ cutoff**:Â ModelsÂ can'tÂ accessÂ real-timeÂ information

-Â **NoÂ groundÂ truthÂ labels**:Â UnlikeÂ classificationÂ tasksÂ withÂ clearÂ right/wrongÂ answers,Â languageÂ modelsÂ onlyÂ seeÂ textÂ sequencesÂ withoutÂ factualÂ validation

-Â **NoÂ fact-checkingÂ mechanism**:Â NoÂ built-inÂ processÂ toÂ verifyÂ generatedÂ content

##Â MyÂ Thoughts

###Â Cross-lingualÂ bias

-Â ThereÂ isÂ aÂ biasÂ inÂ trainingÂ dataÂ betweenÂ EnglishÂ andÂ otherÂ languagesÂ (*ThisÂ biasÂ isÂ hardÂ toÂ recognizeÂ andÂ removeÂ becauseÂ itÂ requiresÂ languageÂ expertsÂ toÂ validate,Â whichÂ isÂ time-consuming,Â soÂ IÂ thinkÂ thisÂ processÂ isÂ notÂ widelyÂ appliedÂ whenÂ creatingÂ trainingÂ dataÂ forÂ models*),Â meaningÂ evenÂ whenÂ weÂ mentionÂ theÂ sameÂ concept,Â theÂ modelÂ mayÂ generateÂ slightlyÂ differentÂ responsesÂ inÂ differentÂ languages.Â Sometimes,Â justÂ changingÂ someÂ characters,Â theÂ orderÂ ofÂ words,Â orÂ punctuationÂ canÂ leadÂ toÂ differentÂ meanings.

-Â TheÂ reasonÂ forÂ thisÂ canÂ beÂ theÂ dependenciesÂ ofÂ meaning.Â [WhenÂ youÂ explainÂ aÂ wordÂ inÂ English,Â youÂ actuallyÂ useÂ otherÂ EnglishÂ wordsÂ toÂ explainÂ it](https://www.oxfordlearnersdictionaries.com/definition/english/agnostic_1?q=agnostic).Â TheseÂ wordsÂ wereÂ inventedÂ inÂ countriesÂ likeÂ theÂ US,Â America,Â etc.,Â andÂ haveÂ culturalÂ andÂ historicalÂ backgrounds.Â SoÂ whenÂ youÂ translateÂ themÂ toÂ otherÂ languages,Â theÂ meaningÂ mayÂ beÂ different.

-Â SoÂ theseÂ biasesÂ existedÂ butÂ rarelyÂ peopleÂ knowÂ aboutÂ them.Â TheyÂ leadÂ toÂ inconsistentÂ knowledgeÂ representation,Â whichÂ canÂ causeÂ hallucinations.

ThisÂ thinkingÂ isÂ consolidatedÂ fromÂ [BenchmarkingÂ Concept-SpillingÂ AcrossÂ LanguagesÂ inÂ LLMs](https://arxiv.org/abs/2601.12549).

###Â Post-trainingÂ effectsÂ createÂ personalityÂ traits:

-Â AtÂ theÂ pretrainingÂ stage,Â weÂ createÂ anÂ ["untamedÂ monster"](https://huyenchip.com/2023/05/02/rlhf.html)Â byÂ makingÂ itÂ learnÂ statisticalÂ patternsÂ fromÂ internetÂ dataÂ (trillionsÂ ofÂ tokens).Â DataÂ includesÂ clickbait,Â misinformation,etc.Â ModelsÂ learnÂ toÂ completeÂ textÂ withoutÂ understandingÂ truthÂ vsÂ falsehood.

-Â AtÂ theÂ supervisedÂ fine-tuningÂ (SFT)Â stage,Â modelsÂ areÂ trainedÂ onÂ higher-qualityÂ dataÂ (StackOverflow,Â Quora,Â humanÂ annotations)Â toÂ followÂ instructionsÂ andÂ beÂ "sociallyÂ acceptable".Â CreatesÂ theÂ "naiveÂ expert"Â -Â soundsÂ authoritativeÂ butÂ mayÂ lackÂ underlyingÂ knowledge.

-Â AtÂ theÂ RLHFÂ stage,Â modelsÂ areÂ polishedÂ toÂ beÂ "customer-appropriate"Â usingÂ rewardÂ models,Â butÂ thisÂ introducesÂ biasÂ whereÂ modelsÂ prioritizeÂ agreementÂ toÂ maximizeÂ reward.Â ModelsÂ learnÂ toÂ beÂ helpful,Â confident,Â andÂ definitiveÂ ratherÂ thanÂ uncertain.

TheseÂ thoughtsÂ areÂ consolidatedÂ fromÂ [AIÂ Sycophancy:Â HowÂ UsersÂ FlagÂ andÂ Respond](https://arxiv.org/pdf/2601.10467).Â (*TheyÂ publishedÂ thisÂ paperÂ onÂ arXivÂ onÂ JanÂ 20,Â 2026.Â IÂ hadÂ similarÂ thoughtsÂ independentlyÂ andÂ wroteÂ themÂ down,Â thenÂ wonderedÂ ifÂ anyoneÂ elseÂ inÂ theÂ worldÂ wasÂ thinkingÂ likeÂ meÂ -Â that'sÂ whenÂ IÂ foundÂ thisÂ paper*)

###Â HallucinationÂ AccumulationÂ inÂ Long-TermÂ Task:

####Â ComeÂ fromÂ theÂ modelÂ itself

Sometimes,Â modelsÂ areÂ tooÂ confidentÂ aboutÂ theirÂ answers,Â especiallyÂ whenÂ performingÂ long-termÂ tasksÂ withoutÂ human-in-the-loopÂ (HITL)Â feedbackÂ orÂ usingÂ toolsÂ toÂ verifyÂ intermediateÂ results,Â evenÂ whenÂ theyÂ haveÂ theÂ abilityÂ toÂ doÂ so.Â ForÂ modelsÂ withÂ reasoningÂ ability,Â it'sÂ hardÂ toÂ trackÂ theÂ reasoningÂ processÂ (*TheyÂ thinkÂ tooÂ much,Â sometimesÂ overthinkingÂ andÂ tooÂ fast*),Â soÂ it'sÂ difficultÂ toÂ knowÂ ifÂ theÂ modelÂ isÂ reasoningÂ correctlyÂ orÂ not.Â EspeciallyÂ inÂ long-termÂ tasks,Â modelsÂ canÂ fallÂ intoÂ incorrectÂ hypothesesÂ orÂ misconceptionsÂ andÂ generateÂ manyÂ hallucinations.Â 

ThereÂ areÂ someÂ solutionsÂ toÂ reduceÂ this,Â forÂ exampleÂ [GLMÂ providesÂ thinkingÂ abilitiesÂ toÂ models](https://docs.z.ai/guides/capabilities/thinking-mode)Â byÂ adjustingÂ theÂ thinkingÂ behaviorÂ ofÂ modelsÂ viaÂ specialÂ tokensÂ andÂ trainingÂ data,Â including:

-Â **InterleavedÂ thinking**:Â AllowingÂ GLMÂ toÂ thinkÂ betweenÂ toolÂ callsÂ andÂ afterÂ receivingÂ toolÂ results.Â ThisÂ enablesÂ moreÂ complex,Â step-by-stepÂ reasoning:Â interpretingÂ eachÂ toolÂ outputÂ beforeÂ decidingÂ whatÂ toÂ doÂ next,Â chainingÂ multipleÂ toolÂ callsÂ withÂ reasoningÂ steps,Â andÂ makingÂ finer-grainedÂ decisionsÂ basedÂ onÂ intermediateÂ results.

-Â **PreservedÂ thinking**:Â TheÂ modelÂ canÂ retainÂ reasoningÂ contentÂ fromÂ previousÂ assistantÂ turnsÂ inÂ theÂ context.Â ThisÂ helpsÂ preserveÂ reasoningÂ continuityÂ andÂ conversationÂ integrity,Â improvesÂ modelÂ performance,Â andÂ increasesÂ cacheÂ hitÂ ratesâ€”savingÂ tokensÂ inÂ realÂ tasks.

-Â **Turn-levelÂ thinking**:Â IsÂ aÂ capabilityÂ thatÂ letsÂ youÂ controlÂ reasoningÂ computationÂ onÂ aÂ per-turnÂ basis:Â withinÂ theÂ sameÂ session,Â eachÂ requestÂ canÂ independentlyÂ chooseÂ toÂ enableÂ orÂ disableÂ thinking.

####Â ComeÂ fromÂ theÂ collaborationÂ betweenÂ userÂ andÂ model

TheÂ hallucinationÂ canÂ alsoÂ comeÂ fromÂ theÂ collaborationÂ betweenÂ userÂ andÂ model:

-Â UserÂ providesÂ unsure/misinformationÂ (*ThisÂ isÂ anÂ agnosticÂ behaviorÂ ofÂ usersÂ -Â ifÂ theyÂ knowÂ theÂ informationÂ isÂ false,Â theyÂ won'tÂ provideÂ it,Â butÂ theyÂ don'tÂ knowÂ thatÂ theyÂ don'tÂ know*)Â â†’Â "NaiveÂ well-kindÂ expert"Â acceptsÂ itÂ asÂ truth.

-Â ModelÂ incorporatesÂ thisÂ falseÂ informationÂ intoÂ itsÂ context/reasoning.

-Â ModelÂ buildsÂ uponÂ falseÂ premisesÂ â†’Â generatesÂ moreÂ hallucinationsÂ basedÂ onÂ badÂ foundation.

-Â UserÂ seesÂ confidentÂ "expert"Â responsesÂ â†’Â trustsÂ theÂ modelÂ more.

-Â CycleÂ repeatsÂ â†’Â accumulatedÂ errorsÂ compoundÂ overÂ time.

TheseÂ thoughtsÂ areÂ consolidatedÂ fromÂ [IfÂ YouÂ WantÂ Coherence,Â OrchestrateÂ aÂ TeamÂ ofÂ Rivals:Â Multi-AgentÂ ModelsÂ ofÂ OrganizationalÂ Intelligence](https://arxiv.org/pdf/2601.14351).Â (*Again,Â iÂ hadÂ similarÂ thoughtsÂ independently,Â seemÂ likeÂ whenÂ iÂ haveÂ someÂ thoughts,Â thereÂ areÂ someÂ oneÂ elseÂ inÂ theÂ worldÂ alreadyÂ publicÂ itÂ onÂ arXivÂ ğŸ˜­*)

##Â Tips

SoÂ theÂ tipsÂ toÂ reduceÂ hallucinationÂ are:

-Â UsingÂ fact-checkingÂ mechanisms

-Â ChallengingÂ theÂ model,Â addingÂ rivalÂ perspectives

-Â OrÂ simplyÂ justÂ askingÂ twiceÂ [PromptÂ RepetitionÂ ImprovesÂ Non-ReasoningÂ LLMs](https://arxiv.org/pdf/2512.14982) 

